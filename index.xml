<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Joshua Loftus</title>
    <link>/</link>
    <description>Recent content on Joshua Loftus</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>lastname at nyu.edu (Joshua Loftus)</managingEditor>
    <webMaster>lastname at nyu.edu (Joshua Loftus)</webMaster>
    <lastBuildDate>Tue, 13 Feb 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Model selection bias invalidates significance tests</title>
      <link>/post/model-selection-bias-invalidates-significance-tests/</link>
      <pubDate>Tue, 13 Feb 2018 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/post/model-selection-bias-invalidates-significance-tests/</guid>
      <description>Significance tests and the reproducibility crisis Significance testing may be one of the most popular statistical tools in science. Researchers and journals often treat significance–having a \(p\)-value \(&amp;lt; 0.05\)–as indication that a finding is true and perhaps publishable. But the tests used to compute many of the \(p\)-values people still rely on today were developed over a century ago, when “computer” was still a job title. Now that we have digital computers and it’s standard practice to collect and analyze “big data,” the mathematical assumptions underlying many classical significance tests are being pushed beyond their limits.</description>
    </item>
    
    <item>
      <title></title>
      <link>/page/introstats/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/page/introstats/</guid>
      <description>Introductory statistics       code{white-space: pre;} pre:not([class]) { background-color: white; }  if (window.hljs) { hljs.configure({languages: []}); hljs.initHighlightingOnLoad(); if (document.readyState &amp;&amp; document.readyState === &#34;complete&#34;) { window.setTimeout(function() { hljs.initHighlighting(); }, 0); } }  h1 { font-size: 34px; } h1.title { font-size: 38px; } h2 { font-size: 30px; } h3 { font-size: 24px; } h4 { font-size: 18px; } h5 { font-size: 16px; } h6 { font-size: 12px; } .</description>
    </item>
    
    <item>
      <title>About me</title>
      <link>/page/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/page/about/</guid>
      <description>I&amp;rsquo;m a statistician and data scientist with a broad range of interests including theory, applications, and teaching with the R statistical programming language. My research focuses on common practices in machine learning and data science pipelines and addressing sources and types of error that have previously been overlooked. This includes, for example:
 Developing methods for inference after model selection such as p-values adjusted for selection bias Analyzing the social fairness of machine learning algorithms from a causal perspective  My work has been published in the Annals of Statistics and Advances in Neural Information Processing Systems (NIPS).</description>
    </item>
    
    <item>
      <title>Introductory statistics</title>
      <link>/page/introstats/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/page/introstats/</guid>
      <description>Course description This course examines modern statistical methods as a basis for decision making in the face of uncertainty. Topics include probability theory, discrete and continuous distributions, hypothesis testing, estimation, and statistical quality control. With the aid of computers, these statistical methods are used to analyze data. Also presented are an introduction to statistical models and their application to decision making. Topics include the simple linear regression model, inference in regression analysis, sensitivity analysis, and multiple regression analysis.</description>
    </item>
    
    <item>
      <title>Research</title>
      <link>/page/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/page/research/</guid>
      <description>Causal inference in fairness This work approaches fair machine learning from a causal inference perspective, arguing that determining what is fair is a similar challenge to determining causality.
Press Some of this work was covered in the New Scientist and 36Kr.
Publications  M. J. Kusner, J. R. Loftus, C. Russell, R. Silva. Counterfactual fairness. Advances in Neural Information Processing Systems, 2017. [link]
 C. Russell, M. J. Kusner, J.</description>
    </item>
    
    <item>
      <title>Teaching</title>
      <link>/page/teaching/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/page/teaching/</guid>
      <description>Course pages  Stat-UB.103: Regression and forecasting (6 credits) Stat-UB.3: Regression and forecasting (2 credits) Stats 390: Statistical Consulting Workshop (at Stanford, old page)  Selected links For the R programming language If you haven&amp;rsquo;t already, you&amp;rsquo;ll need to download and install the R language itself, then download and install the free desktop version of RStudio, finally, you may find this short, free book on the basics of R and RStudio helpful for getting started.</description>
    </item>
    
  </channel>
</rss>