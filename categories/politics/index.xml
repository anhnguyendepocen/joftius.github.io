<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Politics on Joshua Loftus</title>
    <link>/categories/politics/</link>
    <description>Recent content in Politics on Joshua Loftus</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>lastname at nyu.edu (Joshua Loftus)</managingEditor>
    <webMaster>lastname at nyu.edu (Joshua Loftus)</webMaster>
    <lastBuildDate>Fri, 23 Feb 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/politics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Algorithmic fairness is as hard as causation</title>
      <link>/post/algorithmic-fairness-is-as-hard-as-causation/</link>
      <pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/post/algorithmic-fairness-is-as-hard-as-causation/</guid>
      <description>What is algorithmic fairness? (Feel free to skip this section if youâ€™re already familiar with the topic.)
Algorithmic fairness is an interdisciplinary research field concerned with the various ways that algorithms may perpetuate or reinforce unfair legacies of our history, and how we might modify the alorithms or systems they are used in to prevent this. For example, if the training data used in a machine learning methods contains patterns caused by things like racism, sexism, ableism, or other types of injustice, then the model may learn those patterns and use them to make predictions and decisions that are unfair.</description>
    </item>
    
  </channel>
</rss>