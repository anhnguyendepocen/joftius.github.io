<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Joshua Loftus</title>
    <link>/post/</link>
    <description>Recent content in Posts on Joshua Loftus</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>lastname at nyu.edu (Joshua Loftus)</managingEditor>
    <webMaster>lastname at nyu.edu (Joshua Loftus)</webMaster>
    <lastBuildDate>Fri, 05 Oct 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Data for good talk at Columbia Data Science Institute</title>
      <link>/post/data-for-good-talk-at-columbia-data-science-institute/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/post/data-for-good-talk-at-columbia-data-science-institute/</guid>
      <description>(Note: links don&amp;rsquo;t work in this preview, click on the post to view).
I&amp;rsquo;m happy to be speaking at 1pm EST today at Columbia University on the topics of causal inference and selection bias in algorithmic fairness. I believe video will be available at the webinar link, and here are my slides.
The talk is based on work described in this survey with my coauthors Matt Kusner, Chris Russell, and Ricardo Silva.</description>
    </item>
    
    <item>
      <title>Russian twitter trolls attacked Bernie too</title>
      <link>/post/russian-twitter-trolls-attacked-bernie-too/</link>
      <pubDate>Tue, 31 Jul 2018 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/post/russian-twitter-trolls-attacked-bernie-too/</guid>
      <description>(Foreign) Political influence campaigns on Twitter You may have seen stories about Twitter accounts operated by Russians attempting to influence the 2016 election in the United States. Much of the reporting that I’ve seen described a Simple Narrative: Russians tried to help trump and hurt Clinton, even supporting Bernie Sanders in order to attack Clinton. I’ve also seen plenty of Democrats on Twitter attacking Sanders over this. I have not seen any stories reporting the fact that many of these bots also attacked Sanders.</description>
    </item>
    
    <item>
      <title>A conditional approach to inference after model selection</title>
      <link>/post/conditional-approach-to-inference-after-model-selection/</link>
      <pubDate>Mon, 26 Feb 2018 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/post/conditional-approach-to-inference-after-model-selection/</guid>
      <description>Overly honest research methods? A high profile case of a scientist retracting multiple papers due to p-hacking is recently gaining new attention due to a BuzzFeed article. Hopefully this will raise awareness and convince some that “keep hammering away at your data until you find want you were expecting” is a poor way to do science. But it’s possible to get things wrong, for the same reason, no matter how well-intentioned we may be.</description>
    </item>
    
    <item>
      <title>Algorithmic fairness is as hard as causation</title>
      <link>/post/algorithmic-fairness-is-as-hard-as-causation/</link>
      <pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/post/algorithmic-fairness-is-as-hard-as-causation/</guid>
      <description>What is algorithmic fairness? (Feel free to skip this section if you’re already familiar with the topic.)
Algorithmic fairness is an interdisciplinary research field concerned with the various ways that algorithms may perpetuate or reinforce unfair legacies of our history, and how we might modify the alorithms or systems they are used in to prevent this. For example, if the training data used in a machine learning methods contains patterns caused by things like racism, sexism, ableism, or other types of injustice, then the model may learn those patterns and use them to make predictions and decisions that are unfair.</description>
    </item>
    
    <item>
      <title>Model selection bias invalidates significance tests</title>
      <link>/post/model-selection-bias-invalidates-significance-tests/</link>
      <pubDate>Tue, 13 Feb 2018 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/post/model-selection-bias-invalidates-significance-tests/</guid>
      <description>Significance tests and the reproducibility crisis Significance testing may be one of the most popular statistical tools in science. Researchers and journals often treat significance–having a \(p\)-value \(&amp;lt; 0.05\)–as indication that a finding is true and perhaps publishable. But the tests used to compute many of the \(p\)-values people still rely on today were developed over a century ago, when “computer” was still a job title. Now that we have digital computers and it’s standard practice to collect and analyze “big data,” the mathematical assumptions underlying many classical significance tests are being pushed beyond their limits.</description>
    </item>
    
  </channel>
</rss>