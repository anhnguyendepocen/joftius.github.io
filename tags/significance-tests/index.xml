<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Significance Tests on Joshua Loftus</title>
    <link>/tags/significance-tests/</link>
    <description>Recent content in Significance Tests on Joshua Loftus</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>lastname at nyu.edu (Joshua Loftus)</managingEditor>
    <webMaster>lastname at nyu.edu (Joshua Loftus)</webMaster>
    <lastBuildDate>Mon, 18 Nov 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/significance-tests/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A concise defense of statistical significance</title>
      <link>/post/a-concise-defense-of-significance/</link>
      <pubDate>Mon, 18 Nov 2019 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/post/a-concise-defense-of-significance/</guid>
      <description>A letter, signed by over 800 scientists and published in Nature called for an end to using p-values to decide whether data refutes or supports a scientific hypothesis. The letter has received widespread coverage and reignited an old debate.
Weaker arguments Most of the objections to p-values or the p &amp;lt; 0.05 threshold in these articles can be summarized into two categories:
 Objections that would apply to any method resulting in a yes/no decision Objections that would apply to any method with yes/no decisions that might be wrong  Banning p-values or &amp;ldquo;p &amp;lt; 0.</description>
    </item>
    
    <item>
      <title>A conditional approach to inference after model selection</title>
      <link>/post/conditional-approach-to-inference-after-model-selection/</link>
      <pubDate>Mon, 26 Feb 2018 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/post/conditional-approach-to-inference-after-model-selection/</guid>
      <description>Overly honest research methods? A high profile case of a scientist retracting multiple papers due to p-hacking is recently gaining new attention due to a BuzzFeed article. Hopefully this will raise awareness and convince some that “keep hammering away at your data until you find want you were expecting” is a poor way to do science. But it’s possible to get things wrong, for the same reason, no matter how well-intentioned we may be.</description>
    </item>
    
    <item>
      <title>Model selection bias invalidates significance tests</title>
      <link>/post/model-selection-bias-invalidates-significance-tests/</link>
      <pubDate>Tue, 13 Feb 2018 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/post/model-selection-bias-invalidates-significance-tests/</guid>
      <description>Significance tests and the reproducibility crisis Significance testing may be one of the most popular statistical tools in science. Researchers and journals often treat significance–having a \(p\)-value \(&amp;lt; 0.05\)–as indication that a finding is true and perhaps publishable. But the tests used to compute many of the \(p\)-values people still rely on today were developed over a century ago, when “computer” was still a job title. Now that we have digital computers and it’s standard practice to collect and analyze “big data,” the mathematical assumptions underlying many classical significance tests are being pushed beyond their limits.</description>
    </item>
    
  </channel>
</rss>