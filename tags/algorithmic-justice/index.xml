<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithmic justice on Joshua Loftus</title>
    <link>/tags/algorithmic-justice/</link>
    <description>Recent content in Algorithmic justice on Joshua Loftus</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>lastname at nyu.edu (Joshua Loftus)</managingEditor>
    <webMaster>lastname at nyu.edu (Joshua Loftus)</webMaster>
    <lastBuildDate>Wed, 12 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/algorithmic-justice/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Counterfactual privilege ICML talk</title>
      <link>/post/counterfactual-privilege-icml-talk/</link>
      <pubDate>Wed, 12 Jun 2019 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/post/counterfactual-privilege-icml-talk/</guid>
      <description>Talk at ICML2019 I’m talking about some new fairness work at ICML, here’s the schedule, slides, poster, and paper. Below is a brief description of one of the new concepts in this work.
 Counterfactual privilege: an asymmetric fairness constraint In previous work (paper, blog post) we described a causal framework for defining and understanding algorithmic fairness. We start with a mathematical model which can be represented as a graph with arrows designating causal relationships between variables, like this example:</description>
    </item>
    
    <item>
      <title>Data for good talk at Columbia Data Science Institute</title>
      <link>/post/data-for-good-talk-at-columbia-data-science-institute/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/post/data-for-good-talk-at-columbia-data-science-institute/</guid>
      <description>(Note: links don’t work in this preview, click on the post to view).
I’m happy to be speaking at 1pm EST today at Columbia University on the topics of causal inference and selection bias in algorithmic fairness. I believe video will be available at the webinar link, and here are my slides.
The talk is based on work described in this survey with my coauthors Matt Kusner, Chris Russell, and Ricardo Silva.</description>
    </item>
    
    <item>
      <title>Algorithmic fairness is as hard as causation</title>
      <link>/post/algorithmic-fairness-is-as-hard-as-causation/</link>
      <pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate>
      <author>lastname at nyu.edu (Joshua Loftus)</author>
      <guid>/post/algorithmic-fairness-is-as-hard-as-causation/</guid>
      <description>What is algorithmic fairness? (Feel free to skip this section if you’re already familiar with the topic.)
Algorithmic fairness is an interdisciplinary research field concerned with the various ways that algorithms may perpetuate or reinforce unfair legacies of our history, and how we might modify the alorithms or systems they are used in to prevent this. For example, if the training data used in a machine learning methods contains patterns caused by things like racism, sexism, ableism, or other types of injustice, then the model may learn those patterns and use them to make predictions and decisions that are unfair.</description>
    </item>
    
  </channel>
</rss>