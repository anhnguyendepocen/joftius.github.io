---
title: "Lecture 25 - Scatterplots and regression lines"
author: "Joshua Loftus"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggthemes)
library(ggrepel)
library(fivethirtyeight)
library(gapminder)
set.seed(1)
```

#### Outline

- Creating scatterplots (and some other cool features)
- Transforming variables / changing scales
- Adding regression lines
- Visualizing "errors"
- What's special about the regression line?
- What IS the regression line?
- Regression vs correlation

## Scatterplots

### This doesn't show all the data! 

```{r}
mpg2008 <- filter(mpg, year == "2008")
mpgplot <- ggplot(mpg2008, aes(cty, hwy)) +
    theme_tufte() + 
    ggtitle("Miles per gallon") +
    geom_text_repel(data = subset(mpg2008, cty >= 26 | cty == 9),
                      mapping = aes(label = model))
mpgplot + geom_point()
```

### Alpha: transparency. Now overlapping points are darker

```{r}
mpgplot + geom_point(alpha = .5)
```

### "Jittering" the points moves them slightly so they don't overlap

```{r}
mpgplot + geom_point() + geom_jitter() +
  geom_smooth(method = "lm", se = FALSE)
```

### "Rug" plots are like 1-dimensional histograms for each axis

```{r}
movies <- bechdel[complete.cases(bechdel),]
movie_plot <- ggplot(movies, aes(budget_2013, intgross_2013)) +
  geom_rug(size = .1, alpha = .2) +
  ggtitle("Budget vs gross earnings") +
  theme_tufte()
movie_plot + geom_point(alpha = .2)
```

## Transforming variables / changing axis scales

```{r}
log_movie_plot <- movie_plot +
  geom_point(alpha = .5, aes(color = binary)) +
  scale_x_log10() + scale_y_log10()
log_movie_plot
```

### Preview of more cool things

```{r}
log_movie_plot + geom_smooth(method = "loess", se = FALSE, color = "black",
                             aes(linetype = binary),
                             data = subset(movies, budget_2013 > 5e5)) +
  geom_abline(intercept = 0, slope = 1)
```

## Regression lines

### We'll go into lots of detail on linear models soon

```{r}
gm2007 <- gapminder %>% filter(year == 2007)
model <- lm(lifeExp ~ gdpPercap, data = gm2007)
log_model <- lm(lifeExp ~ log10(gdpPercap), data = gm2007)
gm2007$predicted <- predict(model)
gm2007$log_pred <- predict(log_model)
other_slope <- 5
other_intercept <- 50
gm2007$some_other_line <- other_slope * log10(gm2007$gdpPercap) + other_intercept
```

### But first, let's visualize them

```{r}
# Create the base plot that we'll manipulate in various ways
gm_plot <- ggplot(gm2007, aes(gdpPercap, lifeExp)) + theme_tufte() +
  ggtitle("GDP and life expectancy") +
  geom_point(alpha = .5) 
```

```{r}
# Label some points
gm_plot + geom_text_repel(data = subset(gm2007, gdpPercap > 45000),
                          mapping = aes(label = country))
# Add a regression line
gm_plot +  geom_smooth(method = "lm", se = FALSE)
```

### The relationship doesn't look very linear...

- Variables involving money, like GDP, are almost always skewed (extreme inequality)
- Transforming them to a logarithmic scale may help
- Axis labels on regular scale: 0, 1, 2, 3, ...
- Axis labels on log scale: 1, 10, 100, 1000, ...

### Transform GDP to log(GDP)

```{r}
# Change the gdp variable scale
gm_plot + scale_x_log10() +
  geom_smooth(method = "lm", se = FALSE)
```

## Errors of the model

```{r}
# Error: vertical distance from point to line
gm_plot + scale_x_log10() +
  geom_smooth(method = "lm", se = FALSE) +
  geom_segment(alpha = .5,
               aes(x=gdpPercap,
                   xend=gdpPercap, 
                   y=lifeExp,
                   yend=log_pred))
```

### Some other line (not the regression one)

```{r}
gm_plot +
  geom_segment(alpha = .5,
               aes(x=gdpPercap,
                   xend=gdpPercap, 
                   y=lifeExp,
                   yend=some_other_line)) +
  geom_abline(slope = other_slope,
              intercept = other_intercept,
              color = "blue", size = 1)  + 
  scale_x_log10()
```

### Do the errors overall seem smaller or larger for this vs the regression line?

## What's special about the regression line?

Let's **square all the errors** and then take the **sum** (SSE = sum of squared errors)

```{r}
gm2007 %>% mutate(
  reg_err = lifeExp - log_pred,
  other_err = lifeExp - some_other_line) %>%
  summarize(
    reg_SSE = sum(reg_err^2),
    other_SSE = sum(other_err^2))
```

### Among all straight lines, the regression line has the smallest possible sum of squared errors

## What IS the regression line?

- Astronomers in the 1700's
- Carl Friedrich Gauss developed it the most, inventing the normal distribution in the process
- The asteroid/dwarf planet Ceres, discovered in 1801, tracked for 40 days, but then disappeared behind the sun
- Gauss predicted where it would be using regression, and astronomers found it using his predictions almost a year after they lost track of it
- (This was a pretty big moment in the history of science / statistics)

- Now we need some mathematical notation.

- Remember the equation for a line? $y = mx + b$
- Slope: $m$
- Intercept: $b$
- We're going to use different notation: $y = \beta_0 + \beta_1 x$

- Data: $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$ assuming $X$ and $Y$ are continuous, these are $n$ points 
- Data usually doesn't fit exactly on a line (unless the *correlation* equals...?)
- So we allow **errors** in the equation
$$
y_i = \beta_0 + \beta_1 x_i + e_i
$$



### The coefficients $\beta_0, \beta_1$ and the errors $e_i$ are unknown

- How could we pick the best coefficients?
- How could we get the smallest errors?
- We'll come back to this next time

## Regression vs correlation

| Regression | Correlation |
------------|---------------
| Slope can be any number | Cor. between -1 and 1 |
| Slope positive | Correlation positive |
| Slope negative | Correlation negative |
| Large slope? | Not necessarily large cor. |
| Points are close to line if... | Correlation is close to -1 or 1 |
| Intercept | No intercept |
| Not symmetric | Symmetric |
| Switch axes? Different line | Switch axes? Same correlation |
| One variable is predictor, other is outcome | No distinction between variables |
| Prediction errors measured vertically | Not usually used for prediction |

