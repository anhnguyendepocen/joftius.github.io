---
title: "Lecture 30 - Model selection"
author: "Joshua Loftus"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggfortify)
library(fivethirtyeight)
library(gapminder)
library(ggthemes)
theme_set(theme_tufte())
options(scipen=999)
options(digits=4)
set.seed(1)
```

#### Outline

- Comparing two nested models with the $F$-test
- Automatic model selection algorithms 
- Complexity and bias-variance tradeoff

## The $F$-test for nested models

- A method that uses hypothesis tests to compare linear models
- The models must be nested: the larger of the two models contains all of the predictor variables in the smaller one, plus some extra variables
- Motivating question: do we really need this larger model, or would the smaller, simpler one be sufficient?
- Larger model, often called the "full" model, with $p$ predictors:
$$
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_q x_{ik} + \cdots + \beta_p x_{ip} + e_i
$$
- Simpler model, often called the "sub" model, with $k < p$ predictors:
$$
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_q x_{ik} + e_i
$$
- The sub-model omits variables $k+1, k+2, \ldots, p$
- (We can always rearrange the variables, so it doesn't matter if we're leaving out the first, last, or variables in the middle)

```{r}
full_model <- lm(winpercent ~ chocolate + 
             fruity + caramel + hard + bar + 
             sugarpercent + pricepercent,
           data = candy_rankings)
sub_model <- lm(winpercent ~ chocolate + sugarpercent,
           data = candy_rankings)
chocolate_model <- lm(winpercent ~ chocolate, data = candy_rankings)
summary(full_model)
summary(sub_model)
```


- How could we compare and choose between these models?

- (Could compare residual standard error or adjusted R-squared)

### Hypothesis testing approach

- We may be interested in choosing between the models using a hypothesis test, e.g. for scientific purposes
- The null hypothesis is that the smaller, simpler model is good enough
- The alternative hypothesis is that **at least one** of the additional variables in the larger model provides a significant improvement
- The residual sum of squares (**RSS**) is
$$
\sum_{i=1}^n (y_i - \hat y_i)^2
$$
- Let's write $RSS_f$ for the residual sum of squares when predictions $\hat y$ are given by the full model, and $RSS_s$ for the sub model
- Note: $RSS_f \leq RSS_s$
- The $F$ statistic for comparing these two models is
$$
F = \frac{\left(\frac{RSS_s - RSS_f}{p - k}\right)}{\left(\frac{RSS_f}{n-p}\right)}
$$
- The observed value is compared to an $F_{p-k, n - p}$ distribution to find a $p$-value

- In `R`, after fitting both models, use the `anova()` function on them like this:

```{r}
anova(sub_model, full_model)
```

- Advantage: hypothesis test framing may be a natural way to answer the scientific or practical question that the regression models are being used to study
- Disadvantage: only for nested models, maybe hypothesis testing isn't relevant and we just want the "best" model

## Automatic model selection algorithms 

- Suppose there are multiple predictor variables, possibly many of them, and we don't know which ones to include in the linear model
- How about letting the computer pick them for us?
- General definition: models with more variables are considered more complex

### Create a sequence of models with increasing complexity

- General idea: start with no variables and add one at a time, each time picking the "best" out of the remaining predictors
- Special cases of this idea, each with a different definition of "best" for the next variable to add: forward stepwise, lasso, elastic net, and others
- Now we just have to pick one model out of a (possibly small) list of models

### Penalize complexity 

- Increasing complexity will make the model fit closer and closer to the data, reducing the RSS
- It's never impressive to make a model fit more closely to data unless there are some other constraints on the model that make it useful
- Common approach: penalize complexity. Instead of just trying to minimize RSS (fit) by itself, try to minimize
$$
\text{fit error} + \text{complexity}
$$
- Typically the fit error is measured by RSS and the complexity is quantified in some way related to the number of variables in the model
- For example, one method, called AIC, is to pick the model with the lowest
$$
n\log(RSS) + 2k
$$
where $k$ is the number of variables in the model.

- This code below shows output for a sequence of models. At the end is a summary of the model chosen by the AIC method.

```{r}
candy <- candy_rankings %>% select(-competitorname)
full_model <- lm(winpercent ~ ., data = candy)
const_model <- lm(winpercent ~ 1, data = candy)
selected_model <- step(const_model,
     scope = list(lower = const_model, upper = full_model),
     direction = "forward")
summary(selected_model)
```
- Check: is the adjusted R-squared for this model better than the one for `sub_model` back near the top of these notes? How much better?

## Complexity and bias-variance tradeoff

- In general, models with greater complexity will have lower bias, but higher variance in their predictions
- See the last plot near the bottom of [this page](http://scott.fortmann-roe.com/docs/BiasVariance.html)

- Example: for a single non-linear relationship (instead of having multiple variables), the degree of nonlinearity is a measure of complexity
- A more "wiggly" curve comes from a more complex model--it can wiggle around to get closer to points in the data, making it less biased but with more variance
- The dashed line below has higher complexity (more variance, less bias)

```{r}
gm2007 <- gapminder %>% filter(year == "2007")
ggplot(gm2007, aes(gdpPercap, lifeExp)) + geom_point() +
  stat_smooth(se = F, method = "loess", span = 1) +
  stat_smooth(se = F, method = "loess", span = 0.2, linetype = 5) 
```
