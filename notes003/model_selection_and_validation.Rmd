---
title: "Model selection and validation"
author: "Joshua Loftus"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(ggfortify)
library(fivethirtyeight)
library(gapminder)
library(glmnet)
library(ggthemes)
theme_set(theme_tufte())
#options(scipen=999)
#options(digits=4)
set.seed(1)
```

#### Outline

- Comparing two nested models with the $F$-test
- Optimization and model selection algorithms
- Overfitting, model complexity, bias-variance tradeoff
- Validation and cross-validation

## The $F$-test for nested models

- A method that uses hypothesis tests to compare linear models
- The models must be nested: the larger of the two models contains all of the predictor variables in the smaller one, plus some extra variables
- Motivating question: do we really need this larger model, or would the smaller, simpler one be sufficient?
- Larger model, often called the "full" model, with $p$ predictors:
$$
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_q x_{ik} + \cdots + \beta_p x_{ip} + e_i
$$
- Simpler model, often called the "sub" model, with $k < p$ predictors:
$$
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_q x_{ik} + e_i
$$
- The sub-model omits variables $k+1, k+2, \ldots, p$
- (We can always rearrange the variables, so it doesn't matter if we're leaving out the first, last, or variables in the middle)

```{r}
full_model <- lm(winpercent ~ chocolate + 
             fruity + caramel + hard + bar + 
             sugarpercent + pricepercent,
           data = candy_rankings)
sub_model <- lm(winpercent ~ chocolate + sugarpercent,
           data = candy_rankings)
chocolate_model <- lm(winpercent ~ chocolate, data = candy_rankings)
tidy(full_model)
tidy(sub_model)
rbind(glance(full_model), glance(sub_model))
```


- How could we compare and choose between these models?

- (Could compare residual standard error or adjusted R-squared)

### Hypothesis testing approach

- We may be interested in choosing between the models using a hypothesis test, e.g. for scientific purposes
- The null hypothesis is that the smaller, simpler model is good enough
- The alternative hypothesis is that **at least one** of the additional variables in the larger model provides a significant improvement
- The residual sum of squares (**RSS**) is
$$
\sum_{i=1}^n (y_i - \hat y_i)^2
$$
- Let's write $RSS_f$ for the residual sum of squares when predictions $\hat y$ are given by the full model, and $RSS_s$ for the sub model
- Note: $RSS_f \leq RSS_s$
- The $F$ statistic for comparing these two models is
$$
F = \frac{\left(\frac{RSS_s - RSS_f}{p - k}\right)}{\left(\frac{RSS_f}{n-p}\right)}
$$
- The observed value is compared to an $F_{p-k, n - p}$ distribution to find a $p$-value

- In `R`, after fitting both models, use the `anova()` function on them like this:

```{r}
anova(sub_model, full_model)
```

- Advantage: hypothesis test framing may be a natural way to answer the scientific or practical question that the regression models are being used to study
- Disadvantage: only for nested models, maybe hypothesis testing isn't relevant and we just want the "best" model

## Optimization and model selection algorithms 

- Optimization is an area of math that develops and analyzes methods for finding maximums or minimums of functions
- [Much of it](https://en.wikipedia.org/wiki/Newton%27s_method) is based on calculus, but [not all](https://en.wikipedia.org/wiki/Bisection_method)
- (It's extremely useful and one of the common prereqs for many of the most successful researchers or users of machine learning / AI / or even statistics. Along with linear algebra, if you have the chance to study optimization and it aligns with your goals I strongly recommend it)

- We have already thought of linear regression as an optimization problem: find the best line (or plane/hyperplane for multiple regression), where best means smallest sum of squared errors

$$
\text{minimize}_{\beta_0, \beta_1} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2
$$

- This notation means to pick the parameters $\beta_0$ and $\beta_1$ to minimize the sum of squared errors
- The sample mean can also be thought of as the solution to an optimization problem: find the best constant to minimize the squared errors

$$
\text{minimize}_{\beta_0} \sum_{i=1}^n (y_i - \beta_0)^2
$$

- Many methods in statistics and machine learning can be described as optimization problems, with possibly more complicated kinds of functions

$$
\text{minimize}_{f} \sum_{i=1}^n (y_i - f(x_i))^2
$$

- The resulting model predicts $y$ using the function $f(x)$:

$$
y_i = f(x_i) + e_i
$$

- So far we have used a linear function: $f(x) = \beta_0 + \beta_1 x$
- A method that has recently become very popular, deep learning, constructs complicated non-linear functions by composing many "layers" of simple non-linear functions

$$
\text{minimize}_{f_1, f_2, f_3} \sum_{i=1}^n (y_i - f_3[f_2(f_1[x_i])])^2
$$
- (If you compose linear functions, the resulting function is also linear, just with different coefficients)
- (If you compose simple non-linear functions, the resulting function can be a very complicated non-linear one)

- The issues we talk about today are not just issues with linear models, but with all kinds of similar methods in stats/ML/data science/AI
- Common elements: data, modeling assumptions about what kinds of functions are reasonable, and algorithms to find a function in that class that "best" fits the data
- Whenever we're trying to get the "best" of something, there is a danger of overfitting...

### Model selection algorithms

- Suppose there are multiple predictor variables, possibly many of them, and we don't know which ones to include in the linear model
- How about letting the computer pick them for us?
- General definition: models with more variables are considered more complex

### Create a sequence of models with increasing complexity

- General idea: start with no variables and add one at a time, each time picking the "best" out of the remaining predictors
- Special cases of this idea, each with a different definition of "best" for the next variable to add: forward stepwise, lasso, elastic net, and others
- Now we just have to pick one model out of a (possibly small) list of models

### Penalize complexity 

- Increasing complexity will make the model fit closer and closer to the data, reducing the RSS
- It's never impressive to make a model fit more closely to data unless there are some other constraints on the model that make it useful
- Common approach: penalize complexity. Instead of just trying to minimize RSS (fit) by itself, try to minimize
$$
\text{fit error} + \text{complexity}
$$
- Typically the fit error is measured by RSS and the complexity is quantified in some way related to the number of variables in the model
- For example, one method, called AIC, is to pick the model with the lowest
$$
n\log(RSS) + 2k
$$
where $k$ is the number of variables in the model.

- This code below shows output for a sequence of models. At the end is a summary of the model chosen by the AIC method.

```{r}
candy <- candy_rankings %>% select(-competitorname)
full_model <- lm(winpercent ~ ., data = candy)
const_model <- lm(winpercent ~ 1, data = candy)
selected_model <- step(const_model,
     scope = list(lower = const_model, upper = full_model),
     direction = "forward")
summary(selected_model)
```
- Check: is the adjusted R-squared for this model better than the one for `sub_model` back near the top of these notes? How much better?

```{r}
rbind(glance(selected_model), glance(sub_model))
```

## Overfitting, model complexity, bias-variance tradeoff

- In general, models with greater complexity will have lower bias, but higher variance in their predictions
- See the last plot near the bottom of [this page](http://scott.fortmann-roe.com/docs/BiasVariance.html)

- Example: for a single non-linear relationship (instead of having multiple variables), the degree of nonlinearity is a measure of complexity
- A more "wiggly" curve comes from a more complex model--it can wiggle around to get closer to points in the data, making it less biased but with more variance
- The dashed line below has higher complexity (more variance, less bias)

```{r}
gm2007 <- gapminder %>% filter(year == "2007")
ggplot(gm2007, aes(gdpPercap, lifeExp)) + geom_point() +
  stat_smooth(se = F, method = "loess", span = 1) +
  stat_smooth(se = F, method = "loess", span = 0.2, linetype = 5) 
```

### What if we don't know how to penalize complexity?

- In a previous example, we used a method called AIC which is a measure of how good the model is that penalizes how many variables it has
- But what if we aren't using a linear model, but some other machine learning method where there is no classical statistical theory developing an analogous version of the AIC?
- There is a general method called validation that works for almost every kind of model
- First, we'll go into more detail about why too much complexity is bad

## Overfitting

- [Google flu trends](https://gking.harvard.edu/files/gking/files/0314policyforumff.pdf)
- [Spurious correlations](http://www.tylervigen.com/spurious-correlations)
- [History example](https://xkcd.com/1122/)
- [Math example](https://i.stack.imgur.com/2AUPV.jpg)

- In our previous example, model complexity was measured using the number of predictor variables
- But it may not always be easy to figure out how to measure or penalize model complexity, so it would be nice to have a more general method to avoid overfitting
- Validation and cross-validation are very general, practical, popular, and especially appropriate if there is a large sample of data 

## Validation

- Split the data into two sets: a **training** set and a **test** set
- Use optimization on the training set to fit models
- Measure the accuracy of fitted models on the test set
- Plot this test accuracy or test error rate as a function of model complexity
- Choose complexity that gives the best test error

- In this example I create a fake data set (simulation) with sample size $n = 300$ and $p = 100$ predictor variables. The outcome variable $y_i = x_{i1} + x_{i2} + x_{i3} + e_i$ only depends on the first 3 out of the 100 predictors. 

```{r}
n <- 300
p <- 100
beta <- c(rep(1, 3), rep(0, p - 3))
X <- matrix(rnorm(n*p), nrow = n)
y <- X %*% beta + rnorm(n)
data <- data.frame(y = y, X = X)
```

- Split the data:

```{r}
train <- data %>% sample_frac(0.5)
test <- setdiff(data, train)
```

- Fit models on training data

```{r}
simple_model <- lm(y ~ X.1 + X.2 + X.3, data = train)
full_model <- lm(y ~ ., data = train) # the . means "every variable in the data"
rbind(glance(simple_model), glance(full_model))
```

- Which model is better, the one with 3 predictors or the one with 100 predictors *including* those 3?

- Evaluate models on the test data:

```{r}
errors <- data.frame(
  model = c(rep("simple", n/2), rep("full", n/2)),
  test_error = c((predict(simple_model, newdata = test) - test$y)^2,
                 (predict(full_model, newdata = test) - test$y)^2))

errors %>% group_by(model) %>% summarize(
  median = median(test_error),
  mean = mean(test_error)
)
```

- This was kind of cheating, because I knew the first 3 variables were the right ones to use in the simple model
- What if we don't know that? Can we use a model selection algorithm to pick the right variables?

## Cross-validation 

- Split the data into $K$ sets, often $K = 5$ or 10, for example. 
- Use one of the sets as a test set, fitting models on the remaining $K-1$ sets and measuring their accuracy on this test set
- Repeat the above for each of the $K$ sets
- There are now $K$ measurements of accuracy for each model
- Average these $K$ measurements
- Plot this average as a function of model complexity


- The code below uses the `glmnet` library, which has a function to use cross-validation to pick variables automatically using a method called the lasso

```{r}
x_train <- as.matrix(train[,-1])
y_train <- train[,1]
x_test <- as.matrix(test[,-1])
y_test <- test[,1]
model <- cv.glmnet(x = x_train, y = y_train, nfolds = 5)
```

- Let's look at a plot of the cross-validation model accuracy as a function of complexity

```{r}
autoplot(model)
```

- What are the fitted coefficients of the best model?

```{r}
coef(model)
```

- Cross-validation and the lasso found the right variables to include in the model! Only the first 3, and the rest of the variables are not included

- As we saw before, the model using only the first three variables has better test error than the model which uses all 100 variables

### Summary

- Many cutting edge methods in ML/AI rely on optimization, just like linear regression
- More complex models have more parameters and/or more complex types of functions to predict the outcome variable
- Too much model complexity can lead to overfitting--for example, including too many predictor variables can make it seem like the model does a better job of prediction...
- Test or validation prediction accuracy is a higher standard
- Keep [this image](https://qph.fs.quoracdn.net/main-qimg-17ec84ff3f63f77f6b368f0eb6ef1890) in mind, and remember bias-variance tradeoff
- Cross-validation is a very useful method (when the sample size is large enough) to automatically pick models with low test error
