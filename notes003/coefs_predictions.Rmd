---
title: "Regression: coefficients and prediction"
author: "Joshua Loftus"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggthemes)
library(ggrepel)
library(fivethirtyeight)
library(gapminder)
options(scipen=999)
set.seed(1)
```

#### Outline

- Equations for a line
- Coefficient estimation
- Predictions, errors, extrapolation
- Coefficient interpretation

## Review

- What is covariance? Correlation?
- How is a regression line different from these?
- What do they have in common?
- Which variable is the outcome?
- How are the errors measured?
- What's special about the regression line?

## Equations for a line

- Since linear regression uses straight lines, it will help to remember these facts about them

- Equation for a line using slope and intercept: $y = mx + b$
- We use different notation: $y = \beta_0 + \beta_1 x$
- $\beta_0$ and $\beta_1$ are often called **coefficients** (in computer science: weights)
- Suppose $(x_0, y_0)$ is a point on the line, so $y_0 = m x_0 + b$
- Equation for a line using slope and point: $y - y_0 = \beta_1(x - x_0)$

- Slope measures change in $y$ when $x$ changes by one unit
- If $x$ changes from $x_0$ to $x_0 + 1$, then $y$ changes from $y_0$ to $y_0 + \beta_1$
- This fact can be used to draw the line
- Another strategy for drawing: find two points on the line and connect them. We'll come back to this later

```{r}
range <- data.frame(x = c(-2,0), y = c(0, 1))
ggplot(range, aes(x, y)) + ylim(c(-3,3)) + xlim(c(-3,3)) +
  geom_point(size = 2) +
  geom_abline(slope = 1/2, intercept = 1, color = "blue") +
  geom_abline(slope = -1, intercept = -1, linetype = 2) +
  theme_minimal()
```
 
## Coefficient estimation

- Data: $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$ assuming $X$ and $Y$ are continuous, these are $n$ points 
- Data usually doesn't fit exactly on a line (unless the *correlation* equals...?)
- So we allow **errors** in the equation for the line
$$
y_i = \beta_0 + \beta_1 x_i + e_i
$$

### The coefficients $\beta_0, \beta_1$ and the errors $e_i$ are unknown

- Everything we've learned so far about **estimation** applies here!
- Goal: use the data to calculate sample estimates of coefficients: $\hat \beta_0$ and $\hat \beta_1$
- Previously we saw some regression lines calculated for us by the `lm` function in `R`
- How does that work? How can we do it ourselves?


```{r}
gm2007 <- filter(gapminder, year == "2007")
ggplot(gm2007, aes(gdpPercap, lifeExp)) +
  geom_point() + theme_minimal() +
  ggtitle("How can we compute the regression line?")
```

### Slope: calculating from sample correlations and standard deviations

- We learned that the slope $\beta_1$ has the same *sign* as the correlation between $x$ and $y$ (both are positive or both are negative)
- Let $r = \text{cor}(x, y)$, and $s_x$ is the sample standard deviation of $x$, likewise for $s_y$
- The exact relationship is this:
$$
\hat \beta_1 =  r \cdot \frac{s_y}{s_x}
$$
- **This is an important relationship to remember**
- We'll come back to it when we think about interpretation 

### Intercept: calculating from sample means

- Now that we know the slope, if we knew a point on the line then we could use the slope + point equation for a line
- Let $\bar x$ and $\bar y$ be the sample means of the $x$ and $y$ variables
- The estimated regression line always passes through one interesting point: the mean of the data $(\bar x, \bar y)$
- So if $\hat \beta_0$ and $\hat \beta_1$ are the sample linear regression coefficients, we know
$$
\bar y = \hat \beta_0 + \hat \beta_1 \bar x
$$
- We can use this to calculate $\hat \beta_0$ if we already know $\hat \beta_1$ and the means:
$$
\hat \beta_0 = \bar y - \hat \beta_1 \bar x
$$

```{r}
gm2007 %>% summarize(
  meanx = mean(gdpPercap),
  meany = mean(lifeExp),
  sdx = sd(gdpPercap),
  sdy = sd(lifeExp),
  r = cor(lifeExp, gdpPercap)
)
```

- Calculating regression coefficients from two variable summary

```{r}
beta1 <- cor(gm2007$lifeExp, gm2007$gdpPercap) * sd(gm2007$lifeExp) / sd(gm2007$gdpPercap)
beta0 <- mean(gm2007$lifeExp) - beta1 * mean(gm2007$gdpPercap)
c(beta0, beta1)
```

- Compare to coefficients calculated by `lm`

```{r}
gm_model <- lm(lifeExp ~ gdpPercap, data = gm2007)
gm_model
```

## Prediction and errors

### For points in the data

- Each data point has an actual outcome $y_i$, but we can also predict the outcome using the regression model
- Predicted values are written as: $\hat y_i = \hat \beta_0 + \hat \beta_1 x_i$
- In other words, plug the value of the predictor variable $x$ into the linear equation
- The error or **residual** is $\hat e_i = y_i - \hat y_i$
- We'll go into more detail about residuals in a future lecture, but for now let's see an example

- What's the predicted life expectancy for the United States?

```{r}
gm2007 %>% 
  filter(country == "United States") %>%
  select(gdpPercap, lifeExp)
```

```{r}
59.6 + 0.000637 * 42951
```

- What's the residual?

```{r}
78.24 - 86.96
```

- Plotting all the predicted values

```{r}
gm2007$lifeExpPrediction <- predict(gm_model)
ggplot(gm2007, aes(gdpPercap, lifeExpPrediction)) +
  geom_point() + theme_tufte()
```

- Plotting all the residuals

```{r}
gm2007$errors <- residuals(gm_model)
qplot(gm2007$errors, bins = 20) + theme_tufte()
ggplot(gm2007, aes(gdpPercap, errors)) +
  geom_point() + 
  geom_hline(yintercept = 0) +
  theme_tufte()
```


### For new points

- Can use the regression line to get predictions for new points, ones that are not in our original data
- A new point may not even have the outcome variable, but we can predict it using just the predictor variable
- But beware! It may not work as well...

- Suppose the population of Wakanda is about 12 million
- GDP: over 90 trillion (an estimate of T'Challa's net worth)
- GDP per capita is then at least 7,500,000
- Predicted life expectancy based on this calculation...

```{r}
59.6 + 0.000637 * (7500000)
```
- It can be dangerous to predict for values **outside the range** of the predictor in the data
- This is called **extrapolation** and it will probably yield larger errors than when predicting on values inside the original data

- Safer, and useful: predicting an outcome for a new point when the predictor variable lies **within the range** seen in the original data. (e.g. for `gdpPercap` between 277 and 49,357)
- Useful because we may not even have the actual outcome variable for a new data point. In this case, we can't calculate a residual

## Interpretation

- Fitted regression line passes through the center of the data $(\bar x, \bar y)$
- The slope $\hat \beta_1$ tells us how the predicted outcome $\hat y$ changes if the predictor variable $x$ changes
- If $x$ increase by one unit, then $\hat y$ increases by $\beta_1$ units
$$
\hat \beta_1 =  r \cdot \frac{s_y}{s_x}
$$
- If $x$ increases by one standard deviation $s_x$, then the predicted outcome $\hat y$ increases by $r$ times the standard deviation $s_y$
- Plugging any value of $x$ into $\hat \beta_0 + \hat \beta_1 x$ gives a predicted outcome, but this may be a bad idea if $x$ is outside the range of the data
- Remember that the fitted regression line comes from an equation using **estimates** $\hat \beta_0, \hat \beta_1$

- A few things to think about:
- What would happen if we gathered a new sample of data and calculated the estimated coefficients again?
- How accurate are the estimates? Does the accuracy depend on sample size?
